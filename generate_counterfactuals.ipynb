{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82ce375-41d5-4526-bdda-c51436c76776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f17ecb7-4cca-4dd8-be38-7d27601007b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/philliph/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3668371e-807e-4a92-a96d-ab650fab2bf7",
   "metadata": {},
   "source": [
    "## Load IMDB training data and split each sentence into candidate prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7321b6a8-aa1e-4621-8186-9db8f14c1077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments(sentence, label, example_index):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    if len(tokens) < 6:    # skip sentences with less than 6 tokens\n",
    "        out_df = None\n",
    "    else:\n",
    "        prompts = [' '.join(tokens[:i]) for i in range(4,len(tokens)+1)]\n",
    "        continuations = [' ' + ' '.join(tokens[i:]) for i in range(4,len(tokens)+1)]\n",
    "        \n",
    "        out_df = pd.DataFrame({'example_index' : example_index, 'prompt' : prompts, 'continuation' : continuations, 'original' : sentence, 'label' : label})\n",
    "    \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7bbfef-c774-4ee8-b809-714f14350a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://github.com/tapilab/aaai-2021-counterfactuals/raw/main/data/ds_imdb_sent.pkl\", \"data/ds_imdb_sent.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad78380c-d316-4fbc-acf5-815d52ac34eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Counterfactual:\n",
    "    def __init__(self, df_train, df_test, moniker):\n",
    "        display(df_train.head(1))\n",
    "        self.moniker = moniker\n",
    "        self.train = df_train\n",
    "        self.test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f62bc38-a430-442e-8301-966a9debeca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ds_imdb_sent.pkl', 'rb') as f:\n",
    "    imdb_sents = pickle.load(f)\n",
    "\n",
    "dataset = {'text' : imdb_sents.train.text.values, 'label' : imdb_sents.train.label.values}\n",
    "\n",
    "positive, negative = [],[]\n",
    "segment_df = []\n",
    "counter = 0\n",
    "for j in range(len(dataset['text'])):\n",
    "    label = 0 if dataset['label'][j] == -1 else 4\n",
    "    sentence = dataset['text'][j]\n",
    "    segment_df.append(get_segments(sentence, label, j))\n",
    "\n",
    "segment_df = pd.concat(segment_df).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08c9171-6886-4af4-ba9d-48f4efda3f1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate polarity of each candidate prompt using the self-explaining-NLP repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb7ad4-035c-4118-a3d7-2a1a1fc9825a",
   "metadata": {},
   "source": [
    "#### Set paths to self-explaining-NLP repository and model files\n",
    "You can download roberta-large from [here](https://huggingface.co/roberta-large/tree/main) and the checkpoints from [here](https://drive.google.com/drive/folders/1RV5OJSzN_7p-YkjkmAhq2vzhouZEtzSS?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1798882f-ae66-473d-89ce-c6a243db1944",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_nlp_path = os.path.abspath('../Self_Explaining_Structures_Improve_NLP_Models')\n",
    "roberta_large_path = os.path.join(se_nlp_path, \"roberta-large\")\n",
    "checkpoint_path = os.path.join(se_nlp_path, \"checkpoints/sst5_checkpoints/large/epoch\\=6-valid_loss\\=0.5880-valid_acc_end\\=0.5909.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c7a07-f8a8-4534-872a-fd507356b587",
   "metadata": {},
   "source": [
    "#### Write candidate prompts for classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06e15100-da8c-4990-8959-4516fc4d6be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(se_nlp_path, 'imdb_sents_train')\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_dir, 'test.txt'), 'w') as fo:\n",
    "    for i in range(segment_df.shape[0]):\n",
    "        fo.write(str(segment_df.iloc[i]['label']) + '\\t' + segment_df.iloc[i]['prompt'] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606cc7ee-28f6-4400-81bf-68c1df776a3d",
   "metadata": {},
   "source": [
    "#### Predict polarity of candidate prompts using the SST-5 classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb7cc2-d080-4a06-a309-12723be4c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = subprocess.run(['predict_prompt_polarity.sh', se_nlp_path, roberta_large_path, checkpoint_path, output_dir], \n",
    "                        cwd='../Self_Explaining_Structures_Improve_NLP_Models', capture_output=True, shell=True)\n",
    "\n",
    "if output.returncode == 0:\n",
    "    print('Prompt predictions complete')\n",
    "else:\n",
    "    raise ValueError('The prompt prediction script exited with a non-zero status. Check output for additional details.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595eaee8-9785-4dc6-8d06-70009c7cba48",
   "metadata": {},
   "source": [
    "#### Load prompt predictions and identify those which are neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a021503-05d3-4dc5-a81d-60dc4583e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = os.path.join(se_nlp_path, \"output.txt\")\n",
    "with open(output_file) as f:\n",
    "    prompt_predictions = f.read().splitlines()\n",
    "\n",
    "prompt_predictions = [i for i in prompt_predictions if '<->' in i]\n",
    "segment_df['prediction'] = [i.split('<->')[1] for i in prompt_predictions]\n",
    "\n",
    "neutral_prompts = segment_df[segment_df['prediction'] == '2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa329c-e73b-4f70-ba6e-d3d16b8a7591",
   "metadata": {},
   "source": [
    "## Identify concepts using CoCo-Ex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78233da-dfe6-4660-81c6-354c5dd638b2",
   "metadata": {},
   "source": [
    "#### Set paths to the CoCo-Ex repository and the Stanford parser\n",
    "You can download the Stanford parser [here](https://nlp.stanford.edu/software/stanford-parser-full-2018-10-17.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "305f9ba5-9be0-4e28-8cc6-32e74a711714",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_ex_path = os.path.abspath('../CoCo-Ex')\n",
    "stanford_parser_path = os.path.join(coco_ex_path, \"StanfordParser-3.9.2/stanford-parser-full-2018-10-17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4eb3c-7141-488f-9c76-409e1d8098ab",
   "metadata": {},
   "source": [
    "#### Output IMDB dataset to csv and run CoCo-Ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5be93100-3b52-4d37-94f8-746191080e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_df = pd.DataFrame(dataset['text'])[0].str.replace('\\n','')\n",
    "coco_df.to_csv(os.path.join(coco_ex_path, 'imdb_sents_train.csv'), index=True, header=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c52482-0577-4d11-abe2-2b8dd8afcbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = subprocess.run(['identify_concepts.sh', coco_ex_path, stanford_parser_path], \n",
    "                        capture_output=True, shell=True)\n",
    "\n",
    "if output.returncode == 0:\n",
    "    print('Concept identification complete')\n",
    "else:\n",
    "    raise ValueError('The concept identification script exited with a non-zero status. Check output for additional details.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5edf1a4-de1e-4552-ab27-d0aba5034684",
   "metadata": {},
   "source": [
    "#### Read CoCo-Ex results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "573b8855-4936-486c-9673-3962fcf21416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39409/1244493383.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  entities_filtered = pd.read_csv(file_path, sep='\\t', header=None, error_bad_lines=False, warn_bad_lines=False, encoding=\"utf-8\")\n",
      "/tmp/ipykernel_39409/1244493383.py:2: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  entities_filtered = pd.read_csv(file_path, sep='\\t', header=None, error_bad_lines=False, warn_bad_lines=False, encoding=\"utf-8\")\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(coco_ex_path, 'imdb_sents_train_filtered.tsv')\n",
    "entities_filtered = pd.read_csv(file_path, sep='\\t', header=None, error_bad_lines=False, warn_bad_lines=False, encoding=\"utf-8\")\n",
    "coco_concepts = []\n",
    "coco_sents = []\n",
    "index = []\n",
    "for i in range(entities_filtered.shape[0]):\n",
    "    if entities_filtered.iloc[i][3] is not np.nan:\n",
    "        concepts = entities_filtered.iloc[i][3][1:-1].split('][')\n",
    "        concepts = list(set([j.split('|')[0] for j in concepts if j.split('|')[1] == 'NP']))\n",
    "        coco_concepts.append(concepts)\n",
    "        coco_sents.append(entities_filtered.iloc[i][2])\n",
    "        index.append(int(entities_filtered.iloc[i][0].replace('text:','')))\n",
    "coco_df = pd.DataFrame({'sent' : coco_sents, 'concepts' : coco_concepts, 'example_index' : index})\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "coco_df['key'] = coco_df['sent'].apply(lambda x: regex.sub('', x.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8dfaa-88b5-4798-b0b8-c612b94f9064",
   "metadata": {},
   "source": [
    "## Prepare prompts and constraint sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76177ba7-1ee4-42ff-ba73-91541ecca3f9",
   "metadata": {},
   "source": [
    "#### Filter neutral prompt continuations to only contain identified concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b684f775-f96f-469e-8568-881a4d503bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_prompts = neutral_prompts.merge(coco_df, how='left', on = 'example_index')\n",
    "neutral_prompts = neutral_prompts[~pd.isna(neutral_prompts['concepts'])]\n",
    "neutral_prompts['original_continuation'] = neutral_prompts['continuation']\n",
    "neutral_prompts['continuation'] = neutral_prompts.apply(lambda x: ' '.join(list(set([i for i in x['original_continuation'].split() if i.lower() in x['concepts']]))), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74599467-5f21-4b86-8e67-fe5767f96853",
   "metadata": {},
   "source": [
    "#### Remove tokens consisting only of punctuation to avoid empty constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa73a1c0-6810-42d1-9e4e-8f1f67625af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_prompts = neutral_prompts.assign(continuation = neutral_prompts['continuation'].apply(lambda x: ' '.join([i for i in x.split() if len(set(i).difference(set(string.punctuation))) > 0])),\n",
    "                                         prompt = neutral_prompts['prompt'].apply(lambda x: ' '.join([i for i in x.split() if len(set(i).difference(set(string.punctuation))) > 0])))\n",
    "neutral_prompts['constraints'] = neutral_prompts['continuation'].apply(lambda x: [[i.lower(), i.title()] for i in x.split() if not i.translate(str.maketrans('', '', string.punctuation)).isdigit() and len(''.join([j for j in i if j in string.printable])) > 0])\n",
    "neutral_prompts = neutral_prompts[neutral_prompts['constraints'].str.len() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e0e2a-bdd3-40b4-855d-1334342dfadb",
   "metadata": {},
   "source": [
    "#### Filter neutral prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f44037c-c4a5-4658-a539-d3ad960b12c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_prompts['length'] = neutral_prompts['prompt'].str.len()\n",
    "neutral_prompts = neutral_prompts[neutral_prompts['original'].str.len() < 256] # ensure the number of characters is less than NueroLogic's max_tgt_length parameter\n",
    "neutral_prompts = neutral_prompts[~neutral_prompts['original'].isin(neutral_prompts[neutral_prompts['original'] == neutral_prompts['prompt']]['original'])]\n",
    "neutral_prompts = neutral_prompts[neutral_prompts['original'].str.len() - neutral_prompts['length'] > 2]\n",
    "neutral_prompts = neutral_prompts[neutral_prompts['prompt'].str.split().str.len() >= 4]\n",
    "neutral_prompts = neutral_prompts.drop_duplicates('prompt')\n",
    "\n",
    "# require continuation to be >= 1 token\n",
    "neutral_prompts = neutral_prompts[neutral_prompts['continuation'].str.split().str.len() >= 1]\n",
    "\n",
    "# keep only the longest neutral prompt\n",
    "neutral_prompts = neutral_prompts.sort_values(['original','length'], ascending = False).drop_duplicates(['original'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d1539-abc2-4985-9029-54efb61bc4ef",
   "metadata": {},
   "source": [
    "#### Create single-token prompts for NeuroCFs-1g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce0ab933-6108-4807-9d27-145ef6e9e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_token_prompts = []\n",
    "for j in set(range(len(dataset['text']))).intersection(coco_df['example_index']):\n",
    "    label = dataset['label'][j]\n",
    "    sentence = dataset['text'][j]\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    prompt = ''\n",
    "    k = 0\n",
    "    while prompt == '':\n",
    "        prompt = tokens[k].translate(str.maketrans('', '', string.punctuation)).translate(str.maketrans('', '', string.digits)).title()\n",
    "        k+=1\n",
    "    continuation = ' ' + ' '.join(tokens[k:])\n",
    "    \n",
    "    # filter constraints to only identified concepts\n",
    "    constraints = [[i.lower(), i.title()] for i in tokens[k:] if i.lower() in coco_df.loc[coco_df['example_index'] == j,'concepts'].values[0] and i not in set(string.punctuation) and not i.translate(str.maketrans('', '', string.punctuation)).isdigit() and len(''.join([j for j in i if j in string.printable])) > 0]\n",
    "    \n",
    "    if len(constraints) > 0:\n",
    "        out_df = pd.DataFrame({'example_index' : j, \n",
    "                               'prompt' : prompt, \n",
    "                               'continuation' : continuation, \n",
    "                               'original' : sentence, \n",
    "                               'constraints' : str(constraints), \n",
    "                               'label' : 0 if label == -1 else 4}, index=[j])\n",
    "        single_token_prompts.append(out_df)\n",
    "\n",
    "single_token_prompts = pd.concat(single_token_prompts).reset_index()\n",
    "single_token_prompts['constraints'] = single_token_prompts['constraints'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea4c5be-f736-4f3e-9cc3-c5afe0079389",
   "metadata": {},
   "source": [
    "## Generate counterfactuals using NeuroLogic Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec808b7-d801-47b1-ab76-366001559b8e",
   "metadata": {},
   "source": [
    "#### Set path to the NeuroLogic Decoding repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8c9545f-5d30-42ad-9499-209bea849204",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurologic_path = os.path.abspath('../neurologic_decoding')\n",
    "lm_steer_path = os.path.join(neurologic_path, 'models/experts/sentiment/large/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea47074-6b3c-4ebf-90e0-24af45d69b27",
   "metadata": {},
   "source": [
    "#### Specify the type of NeuroCFs you want to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c447747f-78b3-49e2-8ac8-80bdc480dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuroCFs_type = 'NeuroCFs-np'\n",
    "# NeuroCFs_type = 'NeuroCFs-1g'\n",
    "\n",
    "if NeuroCFs_type == 'NeuroCFs-np':\n",
    "    prompts_df = neutral_prompts\n",
    "elif NeuroCFs_type == 'NeuroCFs-1g':\n",
    "    prompts_df = single_token_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30ea847-0a67-49e6-b256-02dbf50f6fae",
   "metadata": {},
   "source": [
    "#### Write prompts and constraints sets for NeuroLogic Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1778a2f-0614-47a5-8525-ce780ceb00bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_prompts_neurologic(df, output_file):\n",
    "    prompts = df['prompt']\n",
    "    constraints = df['constraints']\n",
    "    original = df['original']\n",
    "    with open(output_file + '_prompts.txt', 'w') as f:\n",
    "        for item in prompts:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    with open(output_file + '_constraints.json', 'w') as f:\n",
    "        for item in constraints:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "    with open(output_file + '_original.txt', 'w') as f:\n",
    "        for item in original:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9baf8d8-ce0f-4a97-be9a-3f4b48e6ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(neurologic_path, 'dataset/counterfactual/imdb_sents_train/', NeuroCFs_type)\n",
    "Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "write_prompts_neurologic(prompts_df[prompts_df['label'] < 2], os.path.join(data_dir, 'negative'))\n",
    "write_prompts_neurologic(prompts_df[prompts_df['label'] > 2], os.path.join(data_dir, 'positive'))\n",
    "\n",
    "with open(os.path.join(data_dir, 'prompts_df.pkl'), 'wb') as f:\n",
    "    pickle.dump(prompts_df, f)\n",
    "prompts_df.to_csv(os.path.join(data_dir, 'prompts_df.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77bbfd2-7c43-4606-892b-c394bd3776ec",
   "metadata": {},
   "source": [
    "#### Run NeuroLogic Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217014b6-e2d8-4701-9b5c-dd25fcfa741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(neurologic_path, 'output', NeuroCFs_type)\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "polarities = ['positive', 'negative']\n",
    "for p in polarities:\n",
    "    q = list(set(polarities).difference([p]))[0]\n",
    "    prompt_file = os.path.join(data_dir, p + '_prompts.txt')\n",
    "    constraint_file = os.path.join(data_dir, p + '_constraints.json')\n",
    "    output_file = os.path.join(output_dir, p + \"_to_\" + q + \".txt\")\n",
    "    model_dir = os.path.join(lm_steer_path, 'finetuned_gpt2_' + q)\n",
    "    \n",
    "    output = subprocess.run(['run_neurologic.sh', neurologic_path, prompt_file, constraint_file, output_file, model_dir], \n",
    "                            capture_output=True, shell=True)\n",
    "\n",
    "    if output.returncode == 0:\n",
    "        print('Generation of ' + p + ' counterfactuals complete')\n",
    "    else:\n",
    "        raise ValueError('The NeuroLogic Decoding script exited with a non-zero status. Check output for additional details.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a1462-c755-4f19-a1ce-998f5b2c1e33",
   "metadata": {},
   "source": [
    "## Load the generated NeuroCounterfactuals and calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9cc7a4a-349b-4a7b-b568-b832540f1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_neurologic_generations(prompts_file, constraints_file, generations_file, original=None, target=None):        \n",
    "    # load prompts, constraints, and generations\n",
    "    data = []; constraints = []; neuro_gens = []\n",
    "    with open(prompts_file,'rb') as f:\n",
    "        for line in f:\n",
    "            data.append(line.decode(\"utf-8\").replace('\\n',''))\n",
    "    with open(constraints_file,'rb') as f:\n",
    "        for line in f:\n",
    "            constraints.append(json.loads(line))\n",
    "    skip_next = False\n",
    "    with open(generations_file,'rb') as f:\n",
    "        for line in f:\n",
    "            text = line.decode(\"utf-8\").replace('\\n','').strip()\n",
    "            if skip_next:\n",
    "                skip_next = False\n",
    "                continue\n",
    "            elif len(text) == 0:\n",
    "                skip_next = True\n",
    "            else:\n",
    "                neuro_gens.append(text)\n",
    "            \n",
    "    # create data frame\n",
    "    generations = pd.DataFrame({'prompt' : data, 'counterfactual' : neuro_gens, 'constraints' : constraints})\n",
    "    generations['original_ending'] = generations['constraints'].apply(lambda x: ' '.join([j[0] for j in x]))\n",
    "    generations['original'] = generations['prompt'] + ' ' + generations['original_ending']\n",
    "    \n",
    "    if 'positive_prompts' in prompts_file:\n",
    "        generations['original_label'] = 'POSITIVE'\n",
    "        generations['target_label'] = 'NEGATIVE'\n",
    "    elif 'negative_prompts' in prompts_file:\n",
    "        generations['original_label'] = 'NEGATIVE'\n",
    "        generations['target_label'] = 'POSITIVE'\n",
    "    \n",
    "    return generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53366c00-17fc-49a8-9eb5-b6e733233fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generations = []\n",
    "for p in polarities:\n",
    "    q = list(set(polarities).difference([p]))[0]\n",
    "    gens = load_neurologic_generations(prompts_file = os.path.join(data_dir, p + '_prompts.txt'), \n",
    "                                       constraints_file = os.path.join(data_dir, p + '_constraints.json'), \n",
    "                                       generations_file = os.path.join(output_dir, p + '_to_' + q + '.txt'))\n",
    "    generations.append(gens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29543e0b-105d-46aa-951f-c186801d8ad8",
   "metadata": {},
   "source": [
    "#### Save NeuroCounterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "946e928f-eb39-4291-a2d2-aba22723de6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactuals = pd.concat(generations)\n",
    "counterfactuals = counterfactuals.drop_duplicates('counterfactual')\n",
    "counterfactuals['label'] = counterfactuals['target_label'].apply(lambda x: 1 if x == 'POSITIVE' else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fe0905b-7a0f-419e-b703-c9661d8f3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctf_output_dir = os.path.abspath(os.path.join('output', NeuroCFs_type))\n",
    "Path(ctf_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(ctf_output_dir, 'counterfactuals.pkl'), 'wb') as f:\n",
    "    pickle.dump(counterfactuals, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
